{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e937b3-4ed8-405c-b441-fea5d0807620",
   "metadata": {},
   "source": [
    "# Compromis biais-variance\n",
    "Le compromis biais-variance est un résultat fondamental en Machine Learning.\n",
    "Il décompose les erreurs d'un modèle en trois catégories : **le biais inductif, la variance et le bruit**.\n",
    "\n",
    "* Les **biais inductifs** d'un modèle caractérisent l'espace des fonctions apprenables par un modèle.\n",
    "* La **variance** est une mesure de la sensibilité que possède un modèle par rapport aux données utilisées pour l'entraîner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f101dd2-d3f9-4c35-8afb-b8f2b645e3c5",
   "metadata": {},
   "source": [
    "## Biais inductif\n",
    "Il est en fait [impossible d'entraîner un modèle sans biais inductif](https://fr.abcdef.wiki/wiki/Ugly_duckling_theorem).\n",
    "Sans cela, il existerait une infinité de fonctions qui seraient capables de modéliser les relations entres des couples $(x, y)$ quelconques.\n",
    "L'ensemble des biais inductifs forment une contrainte sur les caractéristiques que doivent avoir un modèle.\n",
    "L'étape d'apprentissage se résume alors à la recherche de la meilleure fonction parmis l'espace des fonctions\n",
    "modélisables.\n",
    "\n",
    "### Espace de recherche de fonction\n",
    "Il est important d'avoir une intuition de ce que l'on entends par *espace de recherche de fonction*.\n",
    "\n",
    "On peut décrire le processus d'apprentissage d'un modèle comme une façon de trouver la meilleure fonction qui\n",
    "modélise la relation souhaitée entre nos couples $(x, y)$.\n",
    "Il y a donc au départ de l'apprentissage tout un ensemble de fonctions modélisables par notre modèle.\n",
    "Cet ensemble peut être vu comme un espace de recherche.\n",
    "\n",
    "Par exemple, dans le cas de la régression linéaire, la fonction est décrite par les coefficients de la droite modélisée.\n",
    "Dans ce cas, l'espace des fonctions modélisables est décrit par toutes les fonctions linéaires définies par les valeurs que peuvent prendre les coefficients.\n",
    "Durant l'apprentissage d'une régression linéaire, le modèle va trouver les meilleures valeurs des coefficients afin de minimiser le loss d'entraînement, et donc finir par choisir une fonction dans cet espace.\n",
    "\n",
    "### Caractérisation de l'espace de recherche\n",
    "Les biais expriment des contraintes sur les fonctions que peuvent modéliser nos algorithmes.\n",
    "Un modèle linéaire fait l'hypothèse que la relation $(x, y)$ peut être facilement décrite à l'aide d'une droite.\n",
    "Ainsi, l'espace de recherche ne contient que des fonctions affines.\n",
    "\n",
    "L'espace de recherche définit l'expressivité d'un modèle. Le plus cet espace est divers et complexe, le mieux\n",
    "notre modèle sera capable d'apprendre des relations complexes entre nos couples $(x, y)$.\n",
    "Pour avoir un tel espace de recherche, il faut n'imposer que des biais peut contraignants.\n",
    "On a alors un modèle très maléable.\n",
    "Cependant, on se rend vite compte que si notre modèle est trop maléable, il risquera de facilement sur-apprendre\n",
    "car il aura trop finement adapter son modèle aux données d'entraînement.\n",
    "\n",
    "*Les biais inductifs nous permettent donc choisir la forme de l'espace de fonctions modélisables.\n",
    "Ajuster nos biais permet de contrôler le sur-apprentissage ainsi que le sous-apprentissage.*\n",
    "\n",
    "### Biais courants\n",
    "Le biais le plus courant est celui de **la continuité** : on suppose que si deux points $x$ et $x'$ sont proches dans l'espace, alors\n",
    "il est probable que $f(x)$ et $f(x')$ soient proches l'un de l'autre. Intuitivement, cela revient à dire que si deux images ne diffèrent\n",
    "que de quelques pixels, alors si l'une des deux images représente un chien, la seconde sera très probablement une image de chien.\n",
    "\n",
    "Un autre biais omniprésent est celui de **la régularisation**.\n",
    "La régularisation est en fait une façon de biaiser notre modèle vers des solutions plus simples. C'est aussi une hypothèse que l'on\n",
    "choisit lorsque l'on entraîne le modèle !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ce61a-dc18-40ac-b3b4-35b4652a4dba",
   "metadata": {},
   "source": [
    "## Variance\n",
    "La variance d'un modèle résume son besoin de données pour apprendre au mieux. Plus un modèle a de variance et plus il aura besoin\n",
    "de grosses quantités de données afin de ne pas sur-apprendre.\n",
    "\n",
    "Pour être plus précis, on considère que les données utilisées à l'apprentissage proviennent toutes d'une source aléatoire capable\n",
    "de générer toutes les données possibles d'une distribution fixée. On peut alors échantillonner plusieurs jeux de données à partir de cette source.\n",
    "Cela permet alors d'entraîner autant de modèles qu'il y a de jeux de données.\n",
    "Chaque modèle va converger vers une fonction finale qui peut être évalué sur un ensemble de test commun.\n",
    "Dans ce cas, la variance d'un modèle de ML est mesurée par la variance des performances des différents modèles évalués sur le jeu de test,\n",
    "mais entraînés sur des données différentes.\n",
    "\n",
    "Si les prédictions d'un modèle sont trop dépendantes des données utilisées pour l'entraîner, alors c'est qu'il est en sur-apprentissage,\n",
    "car cela signifie qu'il s'est trop spécialisé sur les données fournies d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a20018-9d3e-4046-96f4-ea8748e334e9",
   "metadata": {},
   "source": [
    "## Exemple: régression linéaire vs KNN\n",
    "On va illustrer l'impact des biais et de la variance dans un exemple en utilisant un jeu de données non linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098e163b-85a8-44ac-80e0-e1923280f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def make_dataset(n_samples: int) -> tuple:\n",
    "    X, y = make_friedman1(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,\n",
    "        random_state=42,\n",
    "    )\n",
    "    return train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def plot_perf(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = model.score(X_train, y_train)\n",
    "    print(f'Entraînement : {r2:.3f}')\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(f'Test: {r2:.3f}')\n",
    "\n",
    "n_samples_small = 50\n",
    "n_samples_big = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d652f-6dea-4a74-aafc-f5e0f2925cea",
   "metadata": {},
   "source": [
    "### Régression linéaire\n",
    "Le modèle utilisé lors de la régression linéaire ne peut apprendre que des fonctions de la forme $f(x) = ax + b$ (ici nous n'avons qu'une feature $x$).\n",
    "Pour ce modèle, le biais inductif est l'hypothèse que les données peuvent se modéliser sous la forme d'une droite.\n",
    "L'espace de recherche est donc extrêmement contraint ! Mais l'avantage d'un tel modèle c'est qu'il suffit de peu de points\n",
    "pour rapidement converger vers une fonction $f$ de bonne qualité. En effet, il suffit de deux points pour tracer une droite !\n",
    "*Dans la vie réelle, les points sont bruités donc il faut plus de points pour mieux estimer la droite, mais l'idée reste la même.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e96901-7739-4c74-9676-7a1201098950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle linéaire\n",
      "\n",
      "Petit jeu de données (50 exemples)\n",
      "Entraînement : 0.723\n",
      "Test: 0.920\n",
      "\n",
      "Gros jeu de données (10000 exemples)\n",
      "Entraînement : 0.755\n",
      "Test: 0.745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "print('Modèle linéaire')\n",
    "model = LinearRegression()\n",
    "\n",
    "print(f'\\nPetit jeu de données ({n_samples_small} exemples)')\n",
    "X_train, X_test, y_train, y_test = make_dataset(n_samples_small)\n",
    "plot_perf(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f'\\nGros jeu de données ({n_samples_big} exemples)')\n",
    "X_train, X_test, y_train, y_test = make_dataset(n_samples_big)\n",
    "plot_perf(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6c73c-a040-4248-bee8-596f5f094606",
   "metadata": {},
   "source": [
    "### K-NN\n",
    "On peut comparer cet exemple à celui du KNN. Le KNN ne fait que l'hypothèse de continuité, il prédit la valeur d'un point $x$ par rapport\n",
    "aux autres points vus pendant l'entraînement qui sont au voisinage de $x$. C'est l'hypothèse la plus simple qui soit, et elle laisse place à\n",
    "un ensemble de fonctions modélisables énorme. Cela permet de garder un fort potentiel de modélisation, mais ça demande aussi un nombre de données\n",
    "très élevé pour modéliser précisément une fonction en tout point.\n",
    "*En fait, à cause de la [malédiction de la dimension](https://fr.wikipedia.org/wiki/Fl%C3%A9au_de_la_dimension),\n",
    "le nombre de données nécessaires pour couvrir l'ensemble de définition d'un KNN\n",
    "croît exponentiellement avec le nombre de dimensions de nos features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32955614-cdde-47c1-a536-97817e117f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor\n",
      "\n",
      "Petit jeu de données (50 exemples)\n",
      "Entraînement : 0.413\n",
      "Test: 0.107\n",
      "\n",
      "Gros jeu de données (10000 exemples)\n",
      "Entraînement : 0.892\n",
      "Test: 0.853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "print('K-Nearest Neighbor')\n",
    "model = KNeighborsRegressor(5)\n",
    "\n",
    "print(f'\\nPetit jeu de données ({n_samples_small} exemples)')\n",
    "X_train, X_test, y_train, y_test = make_dataset(n_samples_small)\n",
    "plot_perf(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f'\\nGros jeu de données ({n_samples_big} exemples)')\n",
    "X_train, X_test, y_train, y_test = make_dataset(n_samples_big)\n",
    "plot_perf(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18256a-b9eb-454c-96d2-06994467405f",
   "metadata": {},
   "source": [
    "À travers ces deux exemples, on peut voir qu'il peut y avoir des comportements drastiquement différents lors de l'entraînement de nos modèles.\n",
    "Avec peu de points et une hypothèse forte sur la relation entre nos données, on peut utiliser un modèle simple qui va converger sans problème.\n",
    "Cependant, si nous faisons une hypothèse trop forte ou mauvaise, on peut se retrouver avec un modèle trop contraint qui ne pourra pas trouver de bonne façon\n",
    "de modéliser le problème. Il faudra alors trouver (ou tester) de meilleures hypothèses, ou adoucir celles déjà faites. Le cas le plus simple\n",
    "est alors d'entraîner un modèle plus expressif comme le KNN, mais il faut alors assez de données pour que ce dernier soit capable d'apprendre\n",
    "une relation utile sans sur-apprendre.\n",
    "\n",
    "Notre dataset d'exemple n'étant pas linéaire, le biais du modèle linéaire est trop contraignant dans un régime où nous avons\n",
    "suffisamment de données pour modéliser des modèles plus complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fdee0-5771-4da2-baa1-f1cdd6d91f91",
   "metadata": {},
   "source": [
    "## Tradeoff biais-variance\n",
    "Le compromis biais-variance est fondamental en Machine Learning.\n",
    "\n",
    "Intuitivement, on a un équilibre à trouver dans la taille de l'espace des fonctions que peuvent modéliser nos modèles.\n",
    "Si on laisse cet espace être trop grand, alors le modèle va trouver une fonction qui sera très performante sur les données d'entraînement\n",
    "mais qui aura un loss élevé sur de nouvelles données à cause d'un sur-apprentissage sévère.\n",
    "A l'inverse, à trop réduire cet espace, on ne va laisser au modèle que des fonctions sous-efficaces pour modéliser la relation entre nos couples $(x, y)$.\n",
    "\n",
    "Idéalement, on voudrait appliquer uniquement des biais qui correspondent à la nature de la vraie relation entre nos données. Cependant\n",
    "il faut garder en tête que ces biais sont souvent inconnus, nous ne pouvons donc que tester différentes hypothèses afin de regarder\n",
    "ce qui fonctionne le mieux en pratique sur les données à considérer.\n",
    "\n",
    "*Une relation linéaire est peut-être sous-efficace par rapport à la vraie relation de votre couple $(x, y)$, mais elle est peut-être\n",
    "ce que vous aurez de mieux entre le compromis \"biais simplificateur\" vs \"nombre de données\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aead9bc6-b2fb-4619-8993-9f1f5ccd3e6d",
   "metadata": {},
   "source": [
    "### Décomposition de l'erreur d'un modèle\n",
    "Le compromis biais-variance est le résultat d'une décomposition de l'erreur d'un modèle $h$ :\n",
    "\n",
    "$$\n",
    "\\text{loss}_{\\text{test}}(h) = \\text{variance}(h) + \\text{biais}(h)^2 + \\text{bruit}\n",
    "$$\n",
    "\n",
    "Les mauvaises performances du modèle évalué sur le jeu de test s'expliquent par la variance du modèle, son biais et le bruit intrinsèque des données.\n",
    "Comme nous l'avons expliqué jusqu'à présent, plus notre modèle a de biais ou de variance et plus il fera d'erreurs sur le jeu de test.\n",
    "\n",
    "* Pour réduire l'erreur liée à la variance, il faudrait pouvoir entraîner le modèle sur l'ensemble des données imaginables pour la tâche en question.\n",
    "* Pour réduire l'erreur liée au biais, il faudrait connaître parfaitement les caractéristiques de la relation de nos couples $(x, y)$.\n",
    "* *Il est impossible de réduire l'erreur liée au bruit !* Ce bruit décrit la part d'inexplicabilité dans la relation entre $x$ et $y$.\n",
    "\n",
    "Ce que définit cette équation, c'est simplement qu'il est possible de réduire les pertes d'un modèle en réduisant sa variance et son biais.\n",
    "Ce n'est pas une preuve qu'il y a forcément un compromis à faire entre la variance et les biais. Ce compromis est intrinsèque à la définition\n",
    "de ces termes : l'un réduit l'espace de recherche et l'autre l'augmente.\n",
    "\n",
    "> Et le bruit alors ?\n",
    "\n",
    "Il est fort probable les features $x$ ne permettent pas de pleinement expliquer la quantité $y$. Dans ce cas, on aura beau essayer de réduire\n",
    "l'erreur au maximum, on aura toujours une petite erreur liée au bruit.\n",
    "Le bruit peut provenir de deux sources différentes :\n",
    "* Un manque de features pour décrire au mieux la situation qui a mené à $y$.\n",
    "Par exemple, une photo n'est qu'une coupe 2D d'un environnement 3D (voire 4D si l'on inclus le temps).\n",
    "Cette coupe peut être insuffisante pour décrire au mieux ce que l'on souhaite (comment deviner ce que fait une personne qui nous tourne le dos ?).\n",
    "* Un bruit purement stochastique dans la source qui a générée $y$. On peut voir ça comme l'imprécision de la mesure de $y$, ou alors simplement\n",
    "un phénomène aléatoire qui se produit lors de la production de $y$.\n",
    "\n",
    "Ce bruit peut être vu ainsi : lorsque j'observe les mêmes $x$, je vais tout de même obtenir des $y$ différents. Le meilleur modèle possible\n",
    "ne peut alors que prédire la valeur moyenne de $y$ sachant $x$.\n",
    "\n",
    "> Mais alors on ne peut pas avoir de modèle parfait ?\n",
    "\n",
    "Et non ! Un [adage très connu en statistique](https://fr.abcdef.wiki/wiki/All_models_are_wrong)\n",
    "décrit la situation ainsi : **Tous les modèles sont faux, mais certains sont utiles.**\n",
    "\n",
    "Il est impossible en pratique de parfaitement modéliser la relation $(x, y)$. Et on ne parle pas que du bruit irréductible,\n",
    "mais même de la fonction réelle qui décrit au mieux la relation entre $x$ et la valeur moyenne $\\hat y$ sachant $x$.\n",
    "Il faudrait avoir toutes les données possibles (pouvez-vous me trouver toutes les photos de chats et de chiens possibles ?),\n",
    "ou faire les bonnes hypothèses sur la relation afin d'injecter les bons biais au modèle.\n",
    "\n",
    "Enfin, il est intéressant de voir qu'une partie du sur-apprentissage est en fait dû au modèle qui s'affine sur le bruit. Il aura alors\n",
    "réduit les erreurs liées au bruit du jeu d'entraînement, mais il aura appris n'importe quoi !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a4fa1-aff0-419e-aa6e-5d684e3bdfb1",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "On peut imaginer ce que donne un tracé de l'erreur d'un modèle dont on ferait progressivement augmenter\n",
    "la complexité.\n",
    "Petit à petit, l'erreur liée à son biais diminue, mais celle liée à la variance augmente.\n",
    "On peut distinguer alors les deux régimes de sous-apprentissage et de sur-apprentissage.\n",
    "\n",
    ":::{figure-md} biais-variance-fig\n",
    "<img src=\"src/biais_variance.png\" alt=\"tradeoff biais-variance\">\n",
    "\n",
    "Visualisation du compromis biais-variance.\n",
    "Inspiré de l'image libre de droit [ici](https://www.ncbi.nlm.nih.gov/books/NBK543534/figure/ch8.Fig3/).\n",
    ":::\n",
    "\n",
    "En pratique, on préfère prendre un modèle faiblement biaisé et ajouter de la régularisation afin de réduire petit à petit le sur-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc12414-c66e-421f-9011-43c22c047911",
   "metadata": {},
   "source": [
    "### (\\*\\*) Détails mathématiques\n",
    "Soit :\n",
    "* $x, y$ : Des couples de données où $x$ est un ensemble de features et $y$ la valeur à déterminer à partir du vecteur $x$.\n",
    "Ces couples proviennent d'une distribution de probabilité $P$ quelconque.\n",
    "* $D$ : Un jeu de données quelconque constitué de couples $(x_i, y_i)$.\n",
    "$D$ est la réalisation d'un échantillonage de $P$. On note $y_i = y(x_i)$.\n",
    "* $h$ : Un modèle de ML capable d'apprendre à partir d'un jeu de données $D$.\n",
    "Si $h$ a été entraîné sur $D$, on le note $h_D$, et on note ses prédictions $h_D(x)$.\n",
    "\n",
    "On peut alors décomposer le loss moyen d'un modèle $h$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{loss}_{\\text{test}}(h) & = \\text{variance}(h) + \\text{biais}(h)^2 + \\text{bruit}\\\\\n",
    "E_{x, y, D}[(h_D(x) - y)^2] & = E_{x, D}[(h_D(x) - \\bar h(x))^2] + E_x[(\\bar h(x) - \\bar y(x))^2] + E_{x, y}[(\\bar y(x) - y(x))^2]\n",
    "\\end{align}$$\n",
    "\n",
    "Où :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\bar h(x) & = E_D[h_D(x)] \\\\\n",
    "\\bar y(x) & = E_{y, x}[y(x)]\n",
    "\\end{align}$$\n",
    "\n",
    "Explications des valeurs ci-dessus :\n",
    "* $\\bar h(x)$ représente la prédiction moyenne du modèle $h$ lorsqu'on l'entraîne sur tous les datasets $D$ probables provenant de la distribution $P$.\n",
    "* $\\bar y(x)$ représente la valeur moyenne de $y$ associée à $x$. En effet, la valeur de $y$ peut être bruitée ou même ne pas complètement dépendre de $x$,\n",
    "donc il faut prendre en compte que même si l'on mesure deux fois le même $x$, il est possible que l'on se retrouve avec des valeurs de $y$ différentes.\n",
    "* $\\text{loss}_{\\text{test}}(h)$ est la performance moyenne du modèle $h$ entraîné sur l'ensemble de $D$ probables et évalué sur l'ensemble des couples $(x, y)$ probables,\n",
    "provenant de la distribution $P$.\n",
    "* La variance est une mesure de l'écart moyen entre la prédiction d'un modèle entraîné avec un dataset $D$ lambda et\n",
    "la prédiction espérée $\\bar h(x)$ de l'ensemble des datasets.\n",
    "* Le biais est une mesure de l'écart moyen entre la prédiction espérée $\\bar h(x)$ et la valeur espérée $\\bar y(x)$.\n",
    "* Le bruit mesure la variance moyenne entre points $y(x)$ et leur moyenne $\\bar y(x)$\n",
    "\n",
    "Il faut savoir que les modèles $h$ ne sont pas tous égaux dans cette équation. Une fois que l'on choisit d'utiliser un modèle linéaire,\n",
    "le biais sera forcément très fort. Si le biais n'est pas bon (c.a.d. si $\\bar h(x)$ est loin de $\\bar y(x)$), notre modèle aura de la peine à\n",
    "faire diminuer le loss. Au contraire, un réseau de neurones peut plus facilement contrôler la force de son biais à travers plusieurs mécanismes de régularisation.\n",
    "Cela nous permet mieux choisir la force des biais et de la variance de notre modèle.\n",
    "\n",
    "*Si ici on utilise $h$ pour désigner un modèle, c'est parce que l'on considère qu'un modèle est pleinement défini par ses hypothèses.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79057acc-aeba-415b-a0e7-dfa42b74c4b0",
   "metadata": {},
   "source": [
    "## No Free Lunch (NFL)\n",
    "Le théorème du NFL statue qu'aucun algorithme de recherche de solution n'est meilleur que les autres sur tous les problèmes possibles.\n",
    "La recherche d'un unique algorithme qui serait meilleur que tout les autres est donc inutile. Il est nécessaire de faire des hypothèses sur les biais qui seraient\n",
    "intéressants, afin de choisir le ou les modèles de Machine Learning à entraîner.\n",
    "\n",
    "*Gardez cela en tête, il n'est pas possible de faire un modèle universel !*\n",
    "\n",
    "Le résultat sous-jacent est un peu plus profond que cela, et il existe en fait [plusieurs versions](http://no-free-lunch.org) de ce théorème.\n",
    "Vous pouvez creuser le sujet de votre côté si vous le souhaitez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0aa786-2c3a-4032-94c5-503687c0d2bb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* Les biais inductifs représentent toutes les hypothèses que l'on fait pour réduire l'espace de recherche des solutions.\n",
    "* Il est nécessaire de choisir les bons biais qui permettront à un modèle de bien généraliser. Ils évitent le sur-apprentissage.\n",
    "* Ces biais doivent être utilisés avec parcimonie afin d'éviter le sous-apprentissage.\n",
    "* La variance d'un modèle caractérise la sensibilité du modèle au jeu d'entraînement. Elle augmente avec la taille de l'espace de recherche.\n",
    "* Avoir beaucoup de données permet l'utilisation de modèles plus expressifs (avec peu de biais et beaucoup de variance), tout en évitant le sur-apprentissage.\n",
    "Les données supplémentaires peuvent être vues comme un moyen de régulariser l'entraînement.\n",
    "* Il n'existe pas de modèle universel meilleur que tous les autres sur n'importe quel jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29564206-a206-45ee-973b-06d5c8994fa1",
   "metadata": {},
   "source": [
    "## Sources\n",
    "* [Tradeoff biais-variance, Kilian Weinberger, Cornell University](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html)\n",
    "* [Biais inductif, page wiki](https://fr.abcdef.wiki/wiki/Inductive_bias)\n",
    "* [Rasoir d'Ockham, page wiki](https://fr.wikipedia.org/wiki/Rasoir_d%27Ockham)\n",
    "* [Théorème du vilain petit canard, page wiki](https://fr.abcdef.wiki/wiki/Ugly_duckling_theorem)\n",
    "* [Tous les modèles sont faux, page wiki](https://fr.abcdef.wiki/wiki/All_models_are_wrong)\n",
    "* [No Free Lunch, page wiki](https://fr.abcdef.wiki/wiki/No_free_lunch_in_search_and_optimization)\n",
    "* [No Free Lunch Theorems](http://no-free-lunch.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9edaf7-d6ab-4f58-bee4-f3ab7b3315eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cab9db4b504477df71a62a4b8bf17aea5a56b673af41c52bcbfc3e00b7b218e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
