## BERT - Bidirectional Encoder Representations from Transformers

BERT est un mod√®le de repr√©sentation du langage d√©velopp√© par Google AI Language.
Il est capable de traiter plusieurs phrases comme un ensemble de tokens et
produit des embeddings pour chacun d'eux.

La force de ce type de mod√®le r√©side dans sa capacit√© √† produire des embeddings universels de bonne qualit√©.

## Mod√®le de repr√©sentation du langage

Un mod√®le de repr√©sentation du langage transforme une phrase en une repr√©sentation abstraite de la phrase qui peut √™tre utilis√©e pour une vari√©t√© de t√¢ches :

* Reconnaissance d'entit√©s nomm√©es : √âtant donn√©e une phrase, classer les mots de la phrase (en choisissant parmi un ensemble de labels pr√©d√©finis).
* R√©ponse √† une question (t√¢che de classification binaire) : √âtant donn√© une question et une phrase, d√©terminez si la phrase r√©pond √† la question.
* R√©ponse √† une question (traditionnelle) : √âtant donn√© une question, trouver dans un corpus textuel la phrase qui r√©pond √† la question (marquer son d√©but et sa fin).
* Analyse de sentiment : √âtant donn√© une phrase, d√©terminer le score du sentiment (score faible = tristesse, score √©lev√© = heureux).
* Acceptabilit√© linguistique : √âtant donn√© une phrase, d√©terminer s'il s'agit d'une phrase linguistiquement acceptable.
* √âquivalence s√©mantique : pour deux phrases, d√©terminez si elles sont s√©mantiquement √©quivalentes.

## Objectifs du mod√®le BERT

1. Pr√©-entra√Æner un mod√®le de repr√©sentation du langage qui peut √™tre facilement ajust√© pour une vari√©t√© de t√¢ches.  
2. Rendre le transfer-learning pour le NLP aussi accessible que le transfer-learning pour le Computer Vision (le "ImageNET" du langage naturel).

## Utilisation de BERT pour une t√¢che sp√©cifique


Pour utiliser BERT pour une t√¢che sp√©cifique, une couche de sortie sp√©cifique √† la t√¢che doit √™tre ajout√©e √† l'architecture de base de BERT.
De plus, on peut utiliser des tokens sp√©cifiques en input, comme par exemple un token `[CLS]` ayant pour but de classifier l'ensemble de
l'input (cf. Fonctionnement BERT).

Une √©tape de fine-tuning est souvent n√©cessaire afin de sp√©cialiser le mod√®le √† la t√¢che souhait√©e.

### Fonctionnement de BERT :

1. Input BERT : Une phrase = s√©quence de tokens.
2. Output BERT : un √©tat cach√© final par token en input.

### Couche de sortie suppl√©mentaire sp√©cifique √† la t√¢che :

1. Classification au niveau des phrases (par exemple, analyse des sentiments).
Pour ce faire, il faut ajouter un token sp√©cial `[CLS]` en input. On peut ensuite brancher une couche finale de classification en sortie
sur l'embedding qu'a associ√© BERT √† ce token.

2. Classification au niveau du token (par exemple, la reconnaissance des entit√©s nomm√©es), similaire √† la classification au niveau de la phrase, sauf qu'au lieu de faire une seule pr√©diction en utilisant le token masqu√© `[CLS]`,
une pr√©diction est faite pour chacun des √©tats cach√©s finaux correspondant aux labels associ√©s aux diff√©rents tokens.

## Quelle est la particularit√© de BERT ?

Il utilise un mod√®le de repr√©sentation du langage profond bidirectionnel.

### Diff√©rentes approches de la repr√©sentation du mod√®le de langage :

1. Sans contexte :  
    * Chaque mot d'une phrase est transform√© en un embedding de mots qui est ind√©pendant du contexte.

       La repr√©sentation sans contexte d'une phrase est une s√©quence d'embeddings de mots.

       Le mot "avocat" dans la phrase "Je mange un avocat" et le mot "avocat" dans la phrase "Je vais faire appel √† mon avocat" ont la m√™me repr√©sentation par embedding de mots.

2. Contextuel :  

    * Unidirectionnelle : La repr√©sentation d'une phrase d√©pend du contexte ("de gauche √† droite").
       La repr√©sentation contextuelle de chaque mot dans une phrase est impact√©e par les mots pr√©c√©dents mais pas par les mots suivants.
    * Bi-directionnel peu profond : La repr√©sentation d'une phrase est d√©pendante du contexte.
       La repr√©sentation unidirectionnelle de gauche √† droite d'une phrase est combin√©e avec la repr√©sentation unidirectionnelle de droite √† gauche pour former la repr√©sentation finale.
    * BERT (Deep Bi-directional) : La repr√©sentation d'une phrase d√©pend du contexte.
       Tous les mots (tokens) d'une phrase sont consid√©r√©s en m√™me temps.
       La repr√©sentation contextuelle de chaque mot de la phrase est influenc√©e par les mots pr√©c√©dents ET les mots suivants. Maaaagique ! ü™Ñüßô


### Comment entrainer un mod√®le de langage bidirectionnel profond ?

### Fonctionnement global

Le mod√®le est pr√©-entra√Æn√© de mani√®re non-supervis√©e sur un tr√®s grand ensemble de textes.
Le mod√®le BERT repose sur deux proc√©d√©s lors de sa phase de pr√©-entrainement : *Next Sentence Prediction* (NSP) et *Mask Language Modeling* (MLM) que nous pr√©senterons par la suite.

Le NSP est une t√¢che de classifiction binaire o√π le mod√®le doit pr√©dire si une phrase est la suivante d'une autre.
Le MLM demande au mod√®le de pr√©dire les mots manquants d'une phrase. Chaque mot manquant est remplac√© dans la phrase par un token `[MASK]`.

Bien que le NSP (et le MLM) soient utilis√©s pour pr√©-entra√Æner les mod√®les BERT,
nous pouvons utiliser ces m√©thodes exactes pour affiner nos mod√®les afin de mieux comprendre le style sp√©cifique de la langue dans des uses cases pr√©cis.
*Je ne comprends pas cette phrase ?* => *√ßa fait r√©f√©rence au tranfert learning. BERT a √©t√© pr√©-entrain√© sur des t√¢ches de NSP / MLM, je voulais pr√©ciser que l'on peut r√©utiliser ces m√©thodes sur des t√¢ches pr√©cises (NER, analyse de sentiment etc .) en fonction d'une langue donn√©e.*

#### Fonction de perte (MLM)

Cas profond bidirectionnel (BERT) :

* (Contexte) `Le viewer_1 est [MASK] mais le viewer_2 est m√©content.`

* T√¢che de classification : pr√©dire le ou les tokens cibles (`[MASK]`) avec le contexte en entr√©e.

* Un token choisi au hasard est masqu√© et le mod√®le essaie de retrouver le token en se basant sur le contexte.

* La fonction de perte est la log-vraisemblance moyenne de la classification MLM.

#### Pr√©diction de la phrase suivante (NSP) :

* Choisir deux phrases dans le corpus et les concat√©ner.
* Les phrases concat√©n√©es sont donn√©es en entr√©e au mod√®le.
* Etiquette NSP = 1 si B suit A dans le corpus de texte
* Label NSP = 0 si B est une phrase choisie au hasard dans le corpus. 
* L'ensemble d'entra√Ænement est construit de telle sorte qu'il y ait 50% de chaque √©tiquette.
* T√¢che de classification binaire : d√©terminer si B suit A ou non.
* Calculer le score de classification (= score NSP) 

### Exploitation du mod√®le

Le mod√®le pr√©-entra√Æn√© peut-√™tre utilis√© directement ou alors √™tre fine-tun√© pour des t√¢ches sp√©cifiques.
Son pr√©-entra√Ænement et son architecture tr√®s mall√©able le rend tr√®s performant m√™me si on l'utilise pour autre chose que
pour des t√¢ches de NSP et MLM.

Son cas d'utilisation le plus classique est de cr√©er les embeddings des tokens de vos textes. Il n'est pas n√©cessaire de fine-tuner le mod√®le pour ce faire.
En revanche, si vous voulez faire du sentiment analysis par exemple, vous aurez besoin de fine-tuner le mod√®le afin qu'il s'adapte au nouveau r√¥le
du token `[CLS]`. Ce faisant, vous changerez la fa√ßon dont le mod√®le traite une phrase et la fa√ßon dont il va contextualiser le token `[CLS]` avec le reste des tokens.