{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcaa4e5",
   "metadata": {},
   "source": [
    "# Modèles de langues\n",
    "\n",
    "## Sommaire\n",
    "* [Modèles N-gram](#modeles-n-gram)\n",
    "   * [Estimation de la probabilité maximale](#estimation-de-la-probabilite-maximale)\n",
    "   * [Lissage de Laplace (Laplace smoothing)](#lissage-de-laplace-laplace-smoothing)\n",
    "   * [Lissage de Good-Turing (Good-Turing smoothing)](#lissage-de-good-turing-good-turing-smoothing)\n",
    "* [Evaluation et Perplexité](#evaluation-et-perplexite)\n",
    "* [Mise en pratique](#mise-en-pratique)\n",
    "   * [Génération de texte](#generation-de-texte)\n",
    "   * [Classification de texte](#classification-de-texte)\n",
    "\n",
    "## Modèles N-gram\n",
    "\n",
    "Dans la continuité de la dernière partie, nous étudierons une autre facette des probabilités conditionnelles *(ou théorie bayésienne)*:\n",
    "\n",
    "La génération de modèles de langues à partir de la fréquence des N derniers mots dans un texte.\n",
    "\n",
    "Un modèle de langue permet d'estimer la probabilité d'obtenir un mot à partir des précédents, *souvenez vous de la **loi des grands nombres** qui stipule que l'on peut exprimer une probabilté comme une fréquence de réalisations*.\n",
    "\n",
    "On parle de modèle N-gram lorsque l'on souhaite entraîner un modèle linguistique à partir des N derniers mots, si on utilise que deux mots on parle de modèle bigram, si on en utilise qu'un seul on parle de modèle unigram.\n",
    "\n",
    "Ici on utilisera la méthode d'estimation de la probabilité maximale ou *Maximum Likelihood Estimation* afin de calculer la probabilité jointe d'obtenir des séquences de mots, ou N-gram.\n",
    "\n",
    "### Estimation de la probabilité maximale\n",
    "\n",
    "La probabilité d'obtenir un mot sachant les N-1 mots précédents est la suivante:\n",
    "\n",
    "$$ P(w_n|w_1, w_2, \\cdots, w_{N-1}) = \\frac{\\#(w_1, w_2, \\cdots, w_{N-1}, w_N)}{\\#(w_1, w_2, \\cdots, w_{N-1})} $$\n",
    "\n",
    "Où:\n",
    "* $\\#(w_1, w_2, \\cdots, w_{N-1}, w_N)$: La fréquence des N dernier mots (N-gram)\n",
    "* $\\#(w_1, w_2, \\cdots, w_{N-1})$: La fréquence des N-1 dernier mots (N-1-gram)\n",
    "\n",
    "Nous connaissons donc la probabilité conditionnelle des variables aléatoires (ici des mots), mais nous ne connaissons pas la probabilité jointe de ces mots $P(w_1, \\cdots, w_Z)$ avec $Z$ un entier représentant la longueur de la séquence de mots.\n",
    "\n",
    "Pour cela nous utiliserons la **règle de chaîne** qui stipule:\n",
    "\n",
    "$$ P(x,y) = P(x|y) . P(y) $$\n",
    "\n",
    "La généralisation à N variables aléatoires nous donne:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(x_1, \\cdots, x_N) & = P\\left(\\bigcap_{k=1}^{N}x_k\\right)\n",
    "\\\\ & = P(x_N \\ | \\ x_1, \\cdots, x_{N-1}) \\ . \\ P(x_1, \\cdots, x_{N-1})\n",
    "\\\\ & = P(x_N \\ | \\ x_1, \\cdots, x_{N-1}) \\ . \\ P(x_{N-1} \\ | \\ x_1, \\cdots, x_{N-2}) \\ . \\ P(x_1, \\cdots, x_{N-2})\n",
    "\\\\ & = P(x_N | x_1, \\cdots, x_{N-1}) \\ . \\ P(x_{N-1} | x_1, \\cdots, x_{N-2}) \\ . \\ \\cdots \\ . \\ P(x_2 | x_1) \\ . \\ P(x_1)\n",
    "\\end{align}$$\n",
    "\n",
    "En d'autres termes:\n",
    "\n",
    "$$P\\left(\\bigcap_{k=1}^{N}x_k\\right) = \\prod_{k=1}^{N}P\\left(x_k|\\bigcap_{j=1}^{k-1}x_j\\right)$$\n",
    "\n",
    "Cependant on peut simplifier cette formule puisque seuls les Z dernier mots comptent:\n",
    "\n",
    "$$ P(x_1, \\cdots, x_N) = P(x_N \\ | \\ x_{N-Z+1}, \\cdots, x_{N-1}) \\ . \\ P(x_{N-1} \\ | \\ x_{N-Z}, \\cdots, x_{N-2}) \\ . \\ \\cdots \\ . \\ P(x_2 \\ | \\ x_1) \\ . \\ P(x_1) $$\n",
    "\n",
    "### Lissage de Laplace (Laplace smoothing)\n",
    "\n",
    "Lorsque l'on entraîne un modèle de langue à calculer les probabilités de chaque mots en fonction des N derniers mots on utilise la formule précédente:\n",
    "\n",
    "$$ P(w_n|w_1, w_2, \\cdots, w_{N-1}) = \\frac{\\#(w_1, w_2, \\cdots, w_{N-1}, w_N)}{\\#(w_1, w_2, \\cdots, w_{N-1})} $$\n",
    "\n",
    "Mais cette méthode a un énorme désavantage; lorsqu'une chaîne de mots spécifique n'est pas contenue dans le corpus d'entraînement alors la probabilité conditionnelle correspondante est de 0, et si un des facteurs est nul le produit des facteurs est nul (*anneau intègre*).\n",
    "\n",
    "#### Add-1 smoothing:\n",
    "\n",
    "Pour régler ce problème on utilise le lissage de Laplace (*ou Laplace Smoothing*), cela consiste à ajouter 1 au numérateur, on suppose alors que chaque séquence de N-gram apparaît au moins une fois, puis on ajoute le cardinal de l'ensemble des N-gram (unique) du corpus **|V|** au dénominateur tel que:\n",
    "\n",
    "$$ P_L(w_N \\ | \\ w_1, w_2, \\cdots, w_{N-1}) = \\frac{\\#(w_1, w_2, \\cdots, w_{N-1}, w_N) + 1}{\\#(w_1, w_2, \\cdots, w_{N-1}) + |V|} $$\n",
    "\n",
    "Cependant cette méthode de lissage est sensible aux distorsions pour de grands corpus, par exemple:\n",
    "\n",
    "$$ P(\\text{to} | \\text{want}) = \\frac{608}{927} = 0.66 $$\n",
    "\n",
    "Avec le Lissage de Laplace cela devient:\n",
    "\n",
    "$$ P_L(\\text{to} | \\text{want}) = \\frac{608+1}{927+1446} = 0.26 $$\n",
    "\n",
    "#### Add-k smoothing:\n",
    "\n",
    "On peut améliorer ce lissage en utilisant le *Add-k smoothing* qui consiste à remplacer la formule par:\n",
    "\n",
    "$$ P_L(w_N \\ | \\ w_1, w_2, \\cdots, w_{N-1}) = \\frac{\\#(w_1, w_2, \\cdots, w_{N-1}, w_N) + k}{\\#(w_1, w_2, \\cdots, w_{N-1}) + k|V|} $$\n",
    "\n",
    "#### Unigram Prior smoothing:\n",
    "\n",
    "Une extension du Add-k smoothing est d'utiliser la probabilié du mot $P(w_i)$ au numérateur et de remplacer **k|V|** par **m** tel que:\n",
    "\n",
    "$$ P_L(w_N \\ | \\ w_1, w_2, \\cdots, w_{N-1}) = \\frac{\\#(w_1, w_2, \\cdots, w_{N-1}, w_N) + mP(w_N)}{\\#(w_1, w_2, \\cdots, w_{N-1}) + m} $$\n",
    "\n",
    "### Lissage de Good-Turing (Good-Turing smoothing)\n",
    "\n",
    "Une autre méthode de lissage plus sophistiquée est le Good-Turing smoothing qui consiste à considérer que la fréquence des variables aléatoires est la même que celles qui ont été observées autant de fois, et considérer que la fréquence des variables aléatoires non observées est la même que celles qui ont été observées qu'une seule fois.\n",
    "\n",
    "Ainsi le nombre total d'observations est:\n",
    "\n",
    "$$ N = \\sum_{c=1}^{\\infty} N_c . c $$\n",
    "\n",
    "Où:\n",
    "\n",
    "* $c$: Nombre de fois qu'un élément x a été observé.\n",
    "* $N_c$: Nombre d'éléments ayant été observés c fois.\n",
    "\n",
    "Et la probabilité d'obtenir un élément non observé est:\n",
    "\n",
    "$$ P_{0}(x) = \\frac{N_1}{N} $$\n",
    "\n",
    "Cependant si on calculait ainsi toutes nos probabilités on briserait une loi élémentaire qui stipule que:\n",
    "\n",
    "$$ \\sum_{x \\in D}P(x) = 1 $$\n",
    "\n",
    "C'est pourquoi on résout le dilemne en calculant la fréquence c de la manière suivante:\n",
    "\n",
    "$$ c^* = \\frac{(c+1)N_{c+1}}{N_c} $$\n",
    "\n",
    "De cette manière on retrouve le nombre total d'observations:\n",
    "\n",
    "$$ N = \\sum_{c=0}^{\\infty}N_c . c^* = \\sum_{c=0}^{\\infty}N_c . c $$\n",
    "\n",
    "Et finalement la probabilité d'obtenir un élément x observé c fois est:\n",
    "\n",
    "$$ P_{c}(x) = \\frac{c^*}{N} $$\n",
    "\n",
    "## Evaluation et Perplexité\n",
    "\n",
    "Comme nous l'avons vu la performance d'un modèle de langue dépend:\n",
    "\n",
    "* Du corpus de textes d'entraînement\n",
    "* De la méthode d'estimation des probabilités conditionnelles (ici on a choisi MLE mais il existe d'autres méthodes)\n",
    "* Du lissage associé à la méthode d'estimation (Laplace, Good-Turing)\n",
    "\n",
    "Ainsi nous devons évaluer les performances de notre modèle, pour cela il y a deux types d'évaluations:\n",
    "\n",
    "* L'évaluation extrinsèque\n",
    "* L'évaluation intrinsèque\n",
    "\n",
    "L'évaluation extrinséque ne consiste pas à évaluer le modèle directement mais à évaluer l'application end-to-end l'utilisant tel que la reconnaissance vocale, la traduction, ou encore la correction automatique...\n",
    "\n",
    "L'évaluation intrinsèque consiste à évaluer le modèle tel quel de manière isolé, nous étudierons uniquement l'évaluation intrinsèque qui nous intéresse ici:\n",
    "\n",
    "La mesure la plus populaire pour un modèle linguistique N-gram est la **Perplexité**, cette mesure permet de savoir à quel point le modèle est bon pour prédire le prochain mot dans les données de validation.\n",
    "\n",
    "Soit W = ($w_1, \\cdots, w_N$) un vecteur de mots dans un corpus de validation (le texte). Alors la perplexité d'un modèle linguistique est:\n",
    "\n",
    "$$ PP(W) = \\sqrt[N]{\\frac{1}{P(w_1, \\cdots, w_N)}} $$\n",
    "\n",
    "Plus cette valeur est basse, meilleur est le modèle.\n",
    "\n",
    "## Mise en pratique\n",
    "\n",
    "### Génération de texte\n",
    "\n",
    "*Cette génération de texte se repose sur les formules vues précédemment: l'estimation de la probabilité maximale avec un lissage de Good-Turing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "286dc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "def text_processing(text):\n",
    "    ''' Return cleaned text for Machine Learning '''\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    NEW_LINE = re.compile('\\n')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^a-z\\' #+_]')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = NEW_LINE.sub(' ',text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub('',text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d40235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCorpus(token, dictTMP, w, n):\n",
    "    #add the last word from previous line\n",
    "    if w != '': token.insert(0, w)\n",
    "    temp = list(ngrams(token, n))\n",
    "    \n",
    "    #count the frequency of the n-gram sentences\n",
    "    for t in temp:\n",
    "        sen = ' '.join(t)\n",
    "        dictTMP[sen] += 1\n",
    "    \n",
    "    #then take out the last 3 words\n",
    "    n2 = len(token)\n",
    "\n",
    "    #store the last few words for the next sentence pairing\n",
    "    if (n2 - n) >= 0:\n",
    "        w1 = token[n2 - (n-1)]\n",
    "    \n",
    "    return w, dictTMP\n",
    "\n",
    "#loads the corpus for the dataset and makes the frequency count of quadgram ,bigram and trigram strings\n",
    "def loadCorpus(files, bi_dict, tri_dict, quad_dict, vocab_dict):\n",
    "\n",
    "    w1 = ''    #for storing the 3rd last word to be used for next token set\n",
    "    w2 = ''    #for storing the 2nd last word to be used for next token set\n",
    "    w3 = ''    #for storing the last word to be used for next token set\n",
    "    token = []\n",
    "    #total no. of words in the corpus\n",
    "    word_len = 0\n",
    "\n",
    "    #open the corpus file and read it line by line\n",
    "    for file_path in files:\n",
    "        with open(file_path,'r', encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "\n",
    "                content = text_processing(line)\n",
    "\n",
    "                token = content.split()\n",
    "                word_len = word_len + len(token)\n",
    "\n",
    "                if not token:\n",
    "                    continue\n",
    "\n",
    "                w3, bi_dict = countCorpus(token, bi_dict, w3, 2)\n",
    "                w2, tri_dict = countCorpus(token, tri_dict, w2, 3)\n",
    "                w1, quad_dict = countCorpus(token, quad_dict, w1, 4)\n",
    "\n",
    "                #add new unique words to the vocaulary set if available\n",
    "                for word in token:\n",
    "                    if word not in vocab_dict:\n",
    "                        vocab_dict[word] = 1\n",
    "                    else:\n",
    "                        vocab_dict[word]+= 1\n",
    "    return word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444f974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for finding the adjusted count c* in Good Turing Smoothing\n",
    "def findGoodTuringAdjustCount(c, k, nc_dict):\n",
    "   \n",
    "    adjust_count = ((((c+1)*(nc_dict[c+1]/nc_dict[c])) - (c*(k+1)*nc_dict[k+1]/nc_dict[1])) /\n",
    "                    (1-((k+1)*nc_dict[k + 1] / nc_dict[1])))\n",
    "    return adjust_count\n",
    "\n",
    "#creates dict for storing probable words with their probabilities for a n-gram sentence\n",
    "def findNgramProbGT(vocab_dict, n0_dict, n1_dict, n1_prob_dict, nc_dict, k, n):\n",
    "    \n",
    "    i = 0\n",
    "    V = len(vocab_dict)\n",
    "    for n1_sen in n1_dict:\n",
    "        n1_token = n1_sen.split()\n",
    "\n",
    "        #(n-1)-gram sentence for key\n",
    "        if(n == 2): n0_sen = n1_token[0]\n",
    "        else: n0_sen = ' '.join(n1_token[:n-1])\n",
    "\n",
    "        #find the probability\n",
    "        #Good Turing smoothing has been used\n",
    "        n1_count = n1_dict[n1_sen]\n",
    "        if(n == 2): n0_count = vocab_dict[n0_sen]\n",
    "        else: n0_count = n0_dict[n0_sen]\n",
    "\n",
    "        if n1_dict[n1_sen] <= k  or (n1_sen not in n1_dict):\n",
    "            n1_count = findGoodTuringAdjustCount( n1_dict[n1_sen], k, nc_dict)\n",
    "        if(n == 2):\n",
    "            if vocab_dict[n0_sen] <= k  or (n0_sen not in vocab_dict):\n",
    "                n0_count = findGoodTuringAdjustCount(vocab_dict[n0_sen], k, nc_dict)\n",
    "        else:\n",
    "            if n0_dict[n0_sen] <= k  or (n0_sen not in n0_dict):\n",
    "                n0_count = findGoodTuringAdjustCount(n0_dict[n0_sen], k, nc_dict)\n",
    "\n",
    "        prob = n1_count / n0_count\n",
    "\n",
    "        #add the (n-1)-gram to the n-gram probabiltity dict\n",
    "        if n0_sen not in n1_prob_dict:\n",
    "            n1_prob_dict[n0_sen] = []\n",
    "            n1_prob_dict[n0_sen].append([prob,n1_token[-1]])\n",
    "        else:\n",
    "            n1_prob_dict[n0_sen].append([prob,n1_token[-1]])\n",
    "            \n",
    "    #sort the probable word acc. to their probabilities\n",
    "    for key in n1_prob_dict:\n",
    "        if len(n1_prob_dict[key])>1:\n",
    "            n1_prob_dict[key] = sorted(n1_prob_dict[key],reverse = True)[:n-1] #Warning ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b558bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression related stuff\n",
    "#calculate best fit line for simple regression \n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "#finds the slope for the best fit line\n",
    "def findBestFitSlope(x,y):\n",
    "    m = ((mean(x)*mean(y)-mean(x*y)) / (mean(x)**2-mean(x**2)))\n",
    "    return m\n",
    "      \n",
    "#finds the intercept for the best fit line\n",
    "def findBestFitIntercept(x,y,m):\n",
    "    c = mean(y) - m*mean(x)\n",
    "    return c\n",
    "\n",
    "## Find the count Nc for quadgrams and trigrams where c > 5\n",
    "#token_len : total no. of ngram tokens\n",
    "def findFrequencyOfFrequencyCount(ngram_dict, k, n, V, token_len):\n",
    "    #for keeping count of 'c' value i.e Nc\n",
    "    nc_dict = {}\n",
    "    #we find the value of Nc,c = 0 by V^n - (total n-gram tokens)\n",
    "    nc_dict[0] = V**n - token_len\n",
    "    #find the count Nc till c = k,we will take k = 5\n",
    "    #find counts for n-gram\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] <= k + 1:\n",
    "            if ngram_dict[key] not in nc_dict:\n",
    "                nc_dict[ ngram_dict[key]] = 1\n",
    "            else:\n",
    "                nc_dict[ ngram_dict[key] ] += 1\n",
    "    \n",
    "    #check if all the values of Nc are there in the nc_dict or not ,if there then return           \n",
    "    val_present = True\n",
    "    for i in range(1,7):\n",
    "        if i not in nc_dict:\n",
    "            val_present = False\n",
    "            break\n",
    "    if val_present == True:\n",
    "        return nc_dict\n",
    "    \n",
    "    #now fill in the values of nc in case it is not there using regression upto c = 6\n",
    "    #we use :[ log(Nc) = blog(c) + a ] as the equation\n",
    "\n",
    "    #we first need to find data for regression that is values(Nc,c) we take 5 data points\n",
    "    data_pts = {}\n",
    "    i = 0\n",
    "    #get first 5 counts value i.e c\n",
    "    #for quadgram\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] not in data_pts:\n",
    "                data_pts[ ngram_dict[key] ] = 1\n",
    "                i += 1\n",
    "        if i >5:\n",
    "            break\n",
    "            \n",
    "    #now get Nc for those c values\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] in data_pts:\n",
    "            data_pts[ ngram_dict[key] ] += 1\n",
    "    \n",
    "    #make x ,y coordinates for regression \n",
    "    x_coor = [ np.log(item) for item in data_pts ]\n",
    "    y_coor = [ np.log( data_pts[item] ) for item in data_pts ]\n",
    "    x = np.array(x_coor, dtype = np.float64)\n",
    "    y = np.array(y_coor , dtype = np.float64)\n",
    "   \n",
    "\n",
    "    #now do regression\n",
    "    #find the slope and intercept for the regression equation\n",
    "    slope_m = findBestFitSlope(x,y)\n",
    "    intercept_c = findBestFitIntercept(x,y,slope_m)\n",
    "\n",
    "    #now find the missing Nc terms and give them value using regression\n",
    "    for i in range(1,(k+2)):\n",
    "        if i not in nc_dict:\n",
    "            nc_dict[i] = (slope_m*i) + intercept_c\n",
    "    \n",
    "    return nc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3e42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase de test:  Ceci est une phrase\n",
      "\n",
      "Prédiction:\n",
      " Ceci est une phrase simple et energique\n"
     ]
    }
   ],
   "source": [
    "#finds the word prediction usinng Backoff\n",
    "def doPredictionBackoffGT(input_sen, bi_dict, tri_dict, quad_dict, bi_prob_dict, tri_prob_dict, quad_prob_dict):\n",
    "    #split the input sentence into tokens\n",
    "    token = input_sen.split()\n",
    "    #if the input sen is found in any ngram then give the most probable word for that ngram\n",
    "    #if not then go to the lower order ngram\n",
    "    if input_sen in quad_prob_dict and quad_prob_dict[input_sen][0][0] > 0:\n",
    "        pred = quad_prob_dict[input_sen][0]\n",
    "    elif ' '.join(token[1:]) in tri_prob_dict and tri_prob_dict[' '.join(token[1:])][0][0]>0:\n",
    "        pred = tri_prob_dict[ ' '.join(token[1:]) ][0]\n",
    "    elif ' '.join(token[2:]) in bi_prob_dict and bi_prob_dict[ ' '.join(token[2:]) ][0][0]>0:\n",
    "        pred = bi_prob_dict[' '.join(token[2:])][0]\n",
    "    else:\n",
    "        pred = []\n",
    "    return pred\n",
    "\n",
    "vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "quad_prob_dict = OrderedDict()\n",
    "tri_prob_dict = OrderedDict()\n",
    "bi_prob_dict = OrderedDict()\n",
    "\n",
    "#load the corpus for the dataset\n",
    "train_files = ['data/corpus_1.txt', 'data/corpus_2.txt', 'data/corpus_3.txt', 'data/corpus_4.txt']\n",
    "#load corpus\n",
    "token_len = loadCorpus(train_files, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "\n",
    "k = 5\n",
    "V = len(vocab_dict)\n",
    "quad_nc_dict = findFrequencyOfFrequencyCount(quad_dict, k, 4, V, len(quad_dict))\n",
    "tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "\n",
    "#create quadgram probability dictionary\n",
    "findNgramProbGT(vocab_dict, tri_dict, quad_dict, quad_prob_dict, quad_nc_dict, k, 4)\n",
    "#create trigram probability dictionary\n",
    "findNgramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k, 3)\n",
    "#create bigram probability dictionary\n",
    "findNgramProbGT(vocab_dict, bi_dict, bi_dict, bi_prob_dict, bi_nc_dict, k, 2)\n",
    "\n",
    "input_sen = \"Ceci est une phrase\"\n",
    "print(\"Phrase de test: \", input_sen)\n",
    "for i in range(3):\n",
    "    temp = input_sen.split()\n",
    "    temp = temp[-3:]\n",
    "    temp = \" \".join(temp)\n",
    "    prediction = doPredictionBackoffGT(temp, bi_dict, tri_dict, quad_dict, bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    if prediction:\n",
    "        input_sen = input_sen+' '+prediction[1]\n",
    "print('\\nPrédiction:\\n', input_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b101e0a",
   "metadata": {},
   "source": [
    "### Classification de texte\n",
    "\n",
    "Pour calculer la probabilité d'obtenir une classe sachant un corpus de texte, on utilise le **théorème de Bayes**:\n",
    "\n",
    "$$ P(C | w_1, \\cdots, w_n) = \\frac{P(C)P(w_1, \\cdots, w_n | C)}{P(w_1, \\cdots, w_n)} $$\n",
    "\n",
    "Or on connaît déjà $P(w_1, \\cdots, w_n)$ que l'on a calculé auparavant.\n",
    "\n",
    "En utilisant la **règle de chaîne** on obtient:\n",
    "\n",
    "$$ P(C)P(w_1, \\cdots, w_n | C) = P(C) . P(w_1|C) . P(w_2 | C, w_1) . \\cdots . P(w_n | C, w_1, \\cdots, w_{N-1})\n",
    "\\\\ \\implies P(w_1, \\cdots, w_n | C) = P(w_1|C) . P(w_2 | C, w_1) . \\cdots . P(w_n | C, w_1, \\cdots, w_{N-1})$$\n",
    "\n",
    "On parle d'algorithme bayésien naïf (naive bayes) lorsque l'on suppose que les mots n'ont pas de corrélation entre eux, cet algorithme est de manière surprenante efficace et transforme la formule en:\n",
    "\n",
    "$$ P(w_1, \\cdots, w_n | C) = P(w_1|C) . P(w_2 | C) . \\cdots . P(w_n | C) = \\prod_{k=1}^{N}P(w_k|C) $$\n",
    "\n",
    "Ainsi on obtient:\n",
    "\n",
    "$$ P(C | w_1, \\cdots, w_N) = \\frac{P(C) . \\prod_{k=1}^{N}P(w_k|C)}{P(w_1, \\cdots, w_n)} $$\n",
    "\n",
    "Ici nous testerons l'efficacité de l'algorithme bayésien naïf sur des SMS en les classants entre spam et normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55facbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def NB_text_processing(text):\n",
    "    ''' Return cleaned text for Machine Learning '''\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    NEW_LINE = re.compile('\\n')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^a-z #+_]')\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "    STEMMER = SnowballStemmer('english')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = NEW_LINE.sub(' ',text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub('',text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ',text)\n",
    "    text = ' '.join([STEMMER.stem(word) for word in text.split() if word not in STOPWORDS])\n",
    "    return text\n",
    "\n",
    "def NB_preprocessing(data):\n",
    "    ''' Return train, validation and test set '''\n",
    "    X = data['v2'].tolist()\n",
    "    X = [NB_text_processing(txt) for txt in X]\n",
    "    cv = CountVectorizer(max_features = 5000)\n",
    "\n",
    "    X = cv.fit_transform(X).toarray()\n",
    "    Y = data['v1'].tolist()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.15,train_size=0.85)\n",
    "    print('train len:', len(x_train))\n",
    "    print('test len:', len(x_test))\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def NB_Model(data):\n",
    "    ''' Create a model based on Naive Bayes '''\n",
    "    x_train, x_test, y_train, y_test = NB_preprocessing(data)\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(cm, classes=['Ham', 'Spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60dea335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv((\"data/spam.csv\"), encoding='latin1')\n",
    "data = data[['v1', 'v2']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f2dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 4736\n",
      "test len: 836\n",
      "Accuracy: 0.9880382775119617\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NB_Model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80715cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
