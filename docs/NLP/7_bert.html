
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>BERT - Bidirectional Encoder Representations from Transformers &#8212; IA-Z</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Computer Vision" href="../Cours_CV/0_intro.html" />
    <link rel="prev" title="Embeddings" href="5_Embedings.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">IA-Z</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../README.html">
   Sommaire
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Apprentissage automatique
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/01%20-%20Pourquoi%20le%20ML%20%26%20information%20gr%C3%A2ce%20%C3%A0%20la%20data.html">
   Pourquoi le Machine Learning ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/02%20-%20Elements%20de%20definition.html">
     Eléments de définition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/03%20-%20Regression%20lineaire.html">
     Introduction à la régression : la régression linéaire
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/05%20-%20Generalisation.html">
     Généralisation d’un modèle de Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/06%20-%20R%C3%A9gularisation%20%26%20tradeoff%20biais-variance%20-%20une%20introduction.html">
     Régularisation &amp; tradeoff biais-variance : une introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/07%20-%20R%C3%A9gularisation%20d%27un%20mod%C3%A8le.html">
     Régularisation d’un modèle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/08%20-%20Compromis%20biais-variance.html">
     Compromis biais-variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Cours%20fondamentaux%20ML/module_1_introduction/11%20-%20Feature%20engineering%20%26%20cleaning.html">
     Feature Engineering
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Traitement automatique de la langue
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Introduction.html">
   Chapitre I: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_DonneesTextuelles.html">
   Etude des données textuelles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_ModStatLanguage.html">
   Chapitre II: Notions générales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_ModLangues.html">
   Modèles de langues
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_Embedings.html">
   Embeddings
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   BERT - Bidirectional Encoder Representations from Transformers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Vision par ordinateur
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/0_intro.html">
   Computer Vision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/1_Image_processing.html">
   Section 1 Image processing techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/2_ML_CV.html">
   Machine Learning for Computer Vision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/3_CNN.html">
   Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/4_Modern_CNN.html">
   Modern Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/5_CV_tasks.html">
   Computer Vision tasks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Apprentissage par renforcement
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20RL/1%20-%20Introduction.html">
   Introduction au Reinforcement learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20RL/3%20-%20Processus%20de%20d%C3%A9cision%20markoviens.html">
   Processus de décision markoviens (MDPs)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Hors-série
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20annexes/mener_une_recherche.html">
   Mener une recherche internet efficacement
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/NLP/7_bert.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ia-z/ia-z"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ia-z/ia-z/issues/new?title=Issue%20on%20page%20%2Fdocs/NLP/7_bert.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   BERT - Bidirectional Encoder Representations from Transformers
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modele-de-representation-du-langage">
   Modèle de représentation du langage
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objectifs-du-modele-bert">
   Objectifs du modèle BERT
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utilisation-de-bert-pour-une-tache-specifique">
   Utilisation de BERT pour une tâche spécifique
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fonctionnement-de-bert">
     Fonctionnement de BERT :
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-de-sortie-supplementaire-specifique-a-la-tache">
     Couche de sortie supplémentaire spécifique à la tâche :
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quelle-est-la-particularite-de-bert">
   Quelle est la particularité de BERT ?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differentes-approches-de-la-representation-du-modele-de-langage">
     Différentes approches de la représentation du modèle de langage :
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comment-entrainer-un-modele-de-langage-bidirectionnel-profond">
     Comment entrainer un modèle de langage bidirectionnel profond ?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fonctionnement-global">
     Fonctionnement global
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fonction-de-perte-mlm">
       Fonction de perte (MLM)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prediction-de-la-phrase-suivante-nsp">
       Prédiction de la phrase suivante (NSP) :
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exploitation-du-modele">
     Exploitation du modèle
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>BERT - Bidirectional Encoder Representations from Transformers</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   BERT - Bidirectional Encoder Representations from Transformers
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modele-de-representation-du-langage">
   Modèle de représentation du langage
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objectifs-du-modele-bert">
   Objectifs du modèle BERT
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utilisation-de-bert-pour-une-tache-specifique">
   Utilisation de BERT pour une tâche spécifique
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fonctionnement-de-bert">
     Fonctionnement de BERT :
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#couche-de-sortie-supplementaire-specifique-a-la-tache">
     Couche de sortie supplémentaire spécifique à la tâche :
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quelle-est-la-particularite-de-bert">
   Quelle est la particularité de BERT ?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differentes-approches-de-la-representation-du-modele-de-langage">
     Différentes approches de la représentation du modèle de langage :
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comment-entrainer-un-modele-de-langage-bidirectionnel-profond">
     Comment entrainer un modèle de langage bidirectionnel profond ?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fonctionnement-global">
     Fonctionnement global
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fonction-de-perte-mlm">
       Fonction de perte (MLM)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prediction-de-la-phrase-suivante-nsp">
       Prédiction de la phrase suivante (NSP) :
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exploitation-du-modele">
     Exploitation du modèle
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="bert-bidirectional-encoder-representations-from-transformers">
<h1>BERT - Bidirectional Encoder Representations from Transformers<a class="headerlink" href="#bert-bidirectional-encoder-representations-from-transformers" title="Permalink to this headline">¶</a></h1>
<p>BERT est un modèle de représentation du langage développé par Google AI Language.
Il est capable de traiter plusieurs phrases comme un ensemble de tokens et
produit des embeddings pour chacun d’eux.</p>
<p>La force de ce type de modèle réside dans sa capacité à produire des embeddings universels de bonne qualité.</p>
</div>
<div class="section" id="modele-de-representation-du-langage">
<h1>Modèle de représentation du langage<a class="headerlink" href="#modele-de-representation-du-langage" title="Permalink to this headline">¶</a></h1>
<p>Un modèle de représentation du langage transforme une phrase en une représentation abstraite de la phrase qui peut être utilisée pour une variété de tâches :</p>
<ul class="simple">
<li><p>Reconnaissance d’entités nommées : Étant donnée une phrase, classer les mots de la phrase (en choisissant parmi un ensemble de labels prédéfinis).</p></li>
<li><p>Réponse à une question (tâche de classification binaire) : Étant donné une question et une phrase, déterminez si la phrase répond à la question.</p></li>
<li><p>Réponse à une question (traditionnelle) : Étant donné une question, trouver dans un corpus textuel la phrase qui répond à la question (marquer son début et sa fin).</p></li>
<li><p>Analyse de sentiment : Étant donné une phrase, déterminer le score du sentiment (score faible = tristesse, score élevé = heureux).</p></li>
<li><p>Acceptabilité linguistique : Étant donné une phrase, déterminer s’il s’agit d’une phrase linguistiquement acceptable.</p></li>
<li><p>Équivalence sémantique : pour deux phrases, déterminez si elles sont sémantiquement équivalentes.</p></li>
</ul>
</div>
<div class="section" id="objectifs-du-modele-bert">
<h1>Objectifs du modèle BERT<a class="headerlink" href="#objectifs-du-modele-bert" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p>Pré-entraîner un modèle de représentation du langage qui peut être facilement ajusté pour une variété de tâches.</p></li>
<li><p>Rendre le transfer-learning pour le NLP aussi accessible que le transfer-learning pour le Computer Vision (le “ImageNET” du langage naturel).</p></li>
</ol>
</div>
<div class="section" id="utilisation-de-bert-pour-une-tache-specifique">
<h1>Utilisation de BERT pour une tâche spécifique<a class="headerlink" href="#utilisation-de-bert-pour-une-tache-specifique" title="Permalink to this headline">¶</a></h1>
<p>Pour utiliser BERT pour une tâche spécifique, une couche de sortie spécifique à la tâche doit être ajoutée à l’architecture de base de BERT.
De plus, on peut utiliser des tokens spécifiques en input, comme par exemple un token <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> ayant pour but de classifier l’ensemble de
l’input (cf. Fonctionnement BERT).</p>
<p>Une étape de fine-tuning est souvent nécessaire afin de spécialiser le modèle à la tâche souhaitée.</p>
<div class="section" id="fonctionnement-de-bert">
<h2>Fonctionnement de BERT :<a class="headerlink" href="#fonctionnement-de-bert" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Input BERT : Une phrase = séquence de tokens.</p></li>
<li><p>Output BERT : un état caché final par token en input.</p></li>
</ol>
</div>
<div class="section" id="couche-de-sortie-supplementaire-specifique-a-la-tache">
<h2>Couche de sortie supplémentaire spécifique à la tâche :<a class="headerlink" href="#couche-de-sortie-supplementaire-specifique-a-la-tache" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Classification au niveau des phrases (par exemple, analyse des sentiments).
Pour ce faire, il faut ajouter un token spécial <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> en input. On peut ensuite brancher une couche finale de classification en sortie
sur l’embedding qu’a associé BERT à ce token.</p></li>
<li><p>Classification au niveau du token (par exemple, la reconnaissance des entités nommées), similaire à la classification au niveau de la phrase, sauf qu’au lieu de faire une seule prédiction en utilisant le token masqué <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>,
une prédiction est faite pour chacun des états cachés finaux correspondant aux labels associés aux différents tokens.</p></li>
</ol>
</div>
</div>
<div class="section" id="quelle-est-la-particularite-de-bert">
<h1>Quelle est la particularité de BERT ?<a class="headerlink" href="#quelle-est-la-particularite-de-bert" title="Permalink to this headline">¶</a></h1>
<p>Il utilise un modèle de représentation du langage profond bidirectionnel.</p>
<div class="section" id="differentes-approches-de-la-representation-du-modele-de-langage">
<h2>Différentes approches de la représentation du modèle de langage :<a class="headerlink" href="#differentes-approches-de-la-representation-du-modele-de-langage" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Sans contexte :</p>
<ul>
<li><p>Chaque mot d’une phrase est transformé en un embedding de mots qui est indépendant du contexte.</p>
<p>La représentation sans contexte d’une phrase est une séquence d’embeddings de mots.</p>
<p>Le mot “avocat” dans la phrase “Je mange un avocat” et le mot “avocat” dans la phrase “Je vais faire appel à mon avocat” ont la même représentation par embedding de mots.</p>
</li>
</ul>
</li>
<li><p>Contextuel :</p>
<ul class="simple">
<li><p>Unidirectionnelle : La représentation d’une phrase dépend du contexte (“de gauche à droite”).
La représentation contextuelle de chaque mot dans une phrase est impactée par les mots précédents mais pas par les mots suivants.</p></li>
<li><p>Bi-directionnel peu profond : La représentation d’une phrase est dépendante du contexte.
La représentation unidirectionnelle de gauche à droite d’une phrase est combinée avec la représentation unidirectionnelle de droite à gauche pour former la représentation finale.</p></li>
<li><p>BERT (Deep Bi-directional) : La représentation d’une phrase dépend du contexte.
Tous les mots (tokens) d’une phrase sont considérés en même temps.
La représentation contextuelle de chaque mot de la phrase est influencée par les mots précédents ET les mots suivants. Maaaagique ! 🪄🧙</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="comment-entrainer-un-modele-de-langage-bidirectionnel-profond">
<h2>Comment entrainer un modèle de langage bidirectionnel profond ?<a class="headerlink" href="#comment-entrainer-un-modele-de-langage-bidirectionnel-profond" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="fonctionnement-global">
<h2>Fonctionnement global<a class="headerlink" href="#fonctionnement-global" title="Permalink to this headline">¶</a></h2>
<p>Le modèle est pré-entraîné de manière non-supervisée sur un très grand ensemble de textes.
Le modèle BERT repose sur deux procédés lors de sa phase de pré-entrainement : <em>Next Sentence Prediction</em> (NSP) et <em>Mask Language Modeling</em> (MLM) que nous présenterons par la suite.</p>
<p>Le NSP est une tâche de classifiction binaire où le modèle doit prédire si une phrase est la suivante d’une autre.
Le MLM demande au modèle de prédire les mots manquants d’une phrase. Chaque mot manquant est remplacé dans la phrase par un token <code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>.</p>
<p>Bien que le NSP (et le MLM) soient utilisés pour pré-entraîner les modèles BERT,
nous pouvons utiliser ces méthodes exactes pour affiner nos modèles afin de mieux comprendre le style spécifique de la langue dans des uses cases précis.
<em>Je ne comprends pas cette phrase ?</em> =&gt; <em>ça fait référence au tranfert learning. BERT a été pré-entrainé sur des tâches de NSP / MLM, je voulais préciser que l’on peut réutiliser ces méthodes sur des tâches précises (NER, analyse de sentiment etc .) en fonction d’une langue donnée.</em></p>
<div class="section" id="fonction-de-perte-mlm">
<h3>Fonction de perte (MLM)<a class="headerlink" href="#fonction-de-perte-mlm" title="Permalink to this headline">¶</a></h3>
<p>Cas profond bidirectionnel (BERT) :</p>
<ul class="simple">
<li><p>(Contexte) <code class="docutils literal notranslate"><span class="pre">Le</span> <span class="pre">viewer_1</span> <span class="pre">est</span> <span class="pre">[MASK]</span> <span class="pre">mais</span> <span class="pre">le</span> <span class="pre">viewer_2</span> <span class="pre">est</span> <span class="pre">mécontent.</span></code></p></li>
<li><p>Tâche de classification : prédire le ou les tokens cibles (<code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>) avec le contexte en entrée.</p></li>
<li><p>Un token choisi au hasard est masqué et le modèle essaie de retrouver le token en se basant sur le contexte.</p></li>
<li><p>La fonction de perte est la log-vraisemblance moyenne de la classification MLM.</p></li>
</ul>
</div>
<div class="section" id="prediction-de-la-phrase-suivante-nsp">
<h3>Prédiction de la phrase suivante (NSP) :<a class="headerlink" href="#prediction-de-la-phrase-suivante-nsp" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Choisir deux phrases dans le corpus et les concaténer.</p></li>
<li><p>Les phrases concaténées sont données en entrée au modèle.</p></li>
<li><p>Etiquette NSP = 1 si B suit A dans le corpus de texte</p></li>
<li><p>Label NSP = 0 si B est une phrase choisie au hasard dans le corpus.</p></li>
<li><p>L’ensemble d’entraînement est construit de telle sorte qu’il y ait 50% de chaque étiquette.</p></li>
<li><p>Tâche de classification binaire : déterminer si B suit A ou non.</p></li>
<li><p>Calculer le score de classification (= score NSP)</p></li>
</ul>
</div>
</div>
<div class="section" id="exploitation-du-modele">
<h2>Exploitation du modèle<a class="headerlink" href="#exploitation-du-modele" title="Permalink to this headline">¶</a></h2>
<p>Le modèle pré-entraîné peut-être utilisé directement ou alors être fine-tuné pour des tâches spécifiques.
Son pré-entraînement et son architecture très malléable le rend très performant même si on l’utilise pour autre chose que
pour des tâches de NSP et MLM.</p>
<p>Son cas d’utilisation le plus classique est de créer les embeddings des tokens de vos textes. Il n’est pas nécessaire de fine-tuner le modèle pour ce faire.
En revanche, si vous voulez faire du sentiment analysis par exemple, vous aurez besoin de fine-tuner le modèle afin qu’il s’adapte au nouveau rôle
du token <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>. Ce faisant, vous changerez la façon dont le modèle traite une phrase et la façon dont il va contextualiser le token <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> avec le reste des tokens.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/NLP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="5_Embedings.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Embeddings</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Cours_CV/0_intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Computer Vision</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Communauté IA-Z<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>