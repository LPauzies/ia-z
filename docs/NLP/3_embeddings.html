
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Embeddings : comment les machines comprennent les mots ? &#8212; IA-Z</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="BERT - Bidirectional Encoder Representations from Transformers" href="4_bert.html" />
    <link rel="prev" title="Etude des données textuelles" href="2_DonneesTextuelles.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">IA-Z</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../README.html">
   Statut
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Apprentissage automatique
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/1%20-%20Pourquoi%20le%20ML%20%26%20information%20gr%C3%A2ce%20%C3%A0%20la%20data.html">
   Pourquoi le Machine Learning ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/2%20-%20Elements%20de%20definition.html">
   Eléments de définition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/3%20-%20Regression%20lineaire.html">
   Modèle élémentaire : La régression linéaire
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/4%20-%20Generalisation.html">
   Généralisation d’un modèle de Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/5%20-%20R%C3%A9gularisation%20%26%20tradeoff%20biais-variance%20-%20une%20introduction.html">
   Régularisation &amp; tradeoff biais-variance : une introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/6%20-%20R%C3%A9gularisation%20d%27un%20mod%C3%A8le.html">
   Régularisation d’un modèle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/7%20-%20Tradeoff%20biais-variance.html">
   Compromis biais-variance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20fondamentaux%20ML/8%20-%20Feature%20engineering%20%26%20cleaning.html">
   Feature Engineering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Traitement automatique de la langue
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Introduction.html">
   Chapitre I: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_DonneesTextuelles.html">
   Etude des données textuelles
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Embeddings : comment les machines comprennent les mots ?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_bert.html">
   BERT - Bidirectional Encoder Representations from Transformers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Vision par ordinateur
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/0_intro.html">
   Computer Vision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/1_Image_processing.html">
   Section 1 Image processing techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/2_ML_CV.html">
   Machine Learning for Computer Vision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/3_CNN.html">
   Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/4_Modern_CNN.html">
   Modern Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours_CV/5_CV_tasks.html">
   Computer Vision tasks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Apprentissage par renforcement
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20RL/intro.html">
   Reinforcement learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Hors-série
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Cours%20annexes/mener_une_recherche.html">
   Mener une recherche internet efficacement
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/NLP/3_embeddings.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ia-z/ia-z"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ia-z/ia-z/issues/new?title=Issue%20on%20page%20%2Fdocs/NLP/3_embeddings.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pre-requis-a-name-prerequis-a">
   “Pré-requis”
   <a name="prerequis">
   </a>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comprendre-versus-lire-un-texte-a-name-introduction-a">
   “Comprendre” versus “Lire un texte”
   <a name="introduction">
   </a>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vecteurs-de-mots-a-name-vecteur2mots-a">
   Vecteurs de mots
   <a name="vecteur2mots">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf-a-name-tfidf-a">
     TF-IDF
     <a name="tfidf">
     </a>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-words-term-frequency-inverse-document-frequency-tf-idf">
     Bag of Words - Term Frequency-Inverse Document Frequency (TF-IDF)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-representation-a-name-1-hot-representation-a">
     One-hot representation
     <a name="1-hot-representation">
     </a>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word2vec-a-name-word2vec-a">
     Word2Vec
     <a name="word2vec">
     </a>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Embeddings : comment les machines comprennent les mots ?</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pre-requis-a-name-prerequis-a">
   “Pré-requis”
   <a name="prerequis">
   </a>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comprendre-versus-lire-un-texte-a-name-introduction-a">
   “Comprendre” versus “Lire un texte”
   <a name="introduction">
   </a>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vecteurs-de-mots-a-name-vecteur2mots-a">
   Vecteurs de mots
   <a name="vecteur2mots">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf-a-name-tfidf-a">
     TF-IDF
     <a name="tfidf">
     </a>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-words-term-frequency-inverse-document-frequency-tf-idf">
     Bag of Words - Term Frequency-Inverse Document Frequency (TF-IDF)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-representation-a-name-1-hot-representation-a">
     One-hot representation
     <a name="1-hot-representation">
     </a>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word2vec-a-name-word2vec-a">
     Word2Vec
     <a name="word2vec">
     </a>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="embeddings-comment-les-machines-comprennent-les-mots">
<h1>Embeddings : comment les machines comprennent les mots ?<a class="headerlink" href="#embeddings-comment-les-machines-comprennent-les-mots" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#prerequis">“Pré-requis”</a></p></li>
<li><p><a class="reference external" href="#introduction">“Comprendre” versus “Lire un texte”</a></p></li>
<li><p><a class="reference external" href="#vecteur2mots">Vecteurs de mots</a></p>
<ol class="simple">
<li><p><a class="reference external" href="#tfidf">Tf-idf</a></p></li>
<li><p><a class="reference external" href="#1-hot-representation">One-hot representation</a></p></li>
<li><p><a class="reference external" href="#word2vec">word2vec</a></p></li>
</ol>
</li>
</ol>
<div class="section" id="pre-requis-a-name-prerequis-a">
<h2>“Pré-requis” <a name="prerequis"></a><a class="headerlink" href="#pre-requis-a-name-prerequis-a" title="Permalink to this headline">¶</a></h2>
<p>Avant de commencer cette section, veillez à bien avoir suivi au préalable le module de Machine Learning (notamment les principes généraux et la partie feature engineering).</p>
</div>
<div class="section" id="comprendre-versus-lire-un-texte-a-name-introduction-a">
<h2>“Comprendre” versus “Lire un texte” <a name="introduction"></a><a class="headerlink" href="#comprendre-versus-lire-un-texte-a-name-introduction-a" title="Permalink to this headline">¶</a></h2>
<p>Depuis longtemps, les machines sont capables de représenter des caractères, des mots, des phrases de manière numérique. L’idée d’utiliser un système de codage binaire pour le langage et la communication remonte au moins à l’invention du télégraphe au XIXe siècle.</p>
<p>L’une des premières formes de codage des langues était le code Morse. Dans ce système, des signaux binaires, comme l’allumage et l’extinction d’une lumière ou l’envoi d’une séquence d’impulsions audio longues et courtes, étaient utilisés pour représenter différents caractères.</p>
<p>Il s’agissait de l’une des méthodes les plus anciennes et les plus simples pour intégrer le langage humain naturel dans un format binaire avec lequel les machines pouvaient travailler d’une certaine manière.</p>
<p>Depuis, de nombreuses améliorations ont été apportées à notre capacité à coder le texte/langage. Il s’agit notamment des enregistrements audio, des nouveaux algorithmes d’encodage tels que l’ASCII, qui utilise un nombre entier pour représenter le texte, et l’Unicode, qui nous permet d’utiliser les caractères de nombreuses écritures différentes.</p>
<p>Votre disque dur n’aura aucun mal à épeler “defend intelligence” un million de fois avec une précision parfaite. Mais pendant la plus grande partie de leur histoire, les ordinateurs n’ont pas été capables de comprendre que le mot “pomme” peut être à la fois une entreprise technologique dont la capitalisation boursière atteint des milliards de dollars et un fruit rouge ou vert juteux.</p>
<p>Lorsque nous voyons un texte, nous voyons plus que l’information brute qui nous est présentée. Les connaissances que nous avons accumulées au fil du temps donnent un contexte supplémentaire aux choses que nous lisons, ce qui rend le langage et la communication significatifs et efficaces.</p>
<p>Il est normal que les ordinateurs ne puissent pas comprendre le texte pour construire des choses intéressantes.</p>
<p>Plusieurs systèmes de NLP basés sur des approches dites <em>rules-based</em> étaient alors jusque là utilisées.
Mais ce type de système basé sur des règles codées en dur est très fragile et ne se généralise pas bien.</p>
<p>A l’inverse des chiffres qui sont dotés de multiples propriétés mathématiques, les mots se prêtent mal à des traitement par des algorithmes. Il faut donc passer par des représentations numériques.</p>
</div>
<div class="section" id="vecteurs-de-mots-a-name-vecteur2mots-a">
<h2>Vecteurs de mots <a name="vecteur2mots"></a><a class="headerlink" href="#vecteurs-de-mots-a-name-vecteur2mots-a" title="Permalink to this headline">¶</a></h2>
<p>Pour récapituler : nous savons que nous pouvons coder du texte brut avec précision en utilisant des méthodes établies comme l’ASCII et l’Unicode. Cependant, nous avons remarqué que le fait de disposer du texte brut ne suffit pas pour créer des modèles NLP. Nous avons donc besoin d’un moyen d’établir une correspondance avec des chiffres qui codent le sens des mots plutôt que l’information brute.</p>
<div class="section" id="tf-idf-a-name-tfidf-a">
<h3>TF-IDF <a name="tfidf"></a><a class="headerlink" href="#tf-idf-a-name-tfidf-a" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="bag-of-words-term-frequency-inverse-document-frequency-tf-idf">
<h3>Bag of Words - Term Frequency-Inverse Document Frequency (TF-IDF)<a class="headerlink" href="#bag-of-words-term-frequency-inverse-document-frequency-tf-idf" title="Permalink to this headline">¶</a></h3>
<p>Le score Term Frequency-Inverse Document Frequency (TF-IDF) permet de représenter un texte au sein d’un corpus sous forme vectorielle. Ces vecteurs sont par la suite utiliser par des modèles de machine learning pour effectuer des prédictions.</p>
<p>Le texte est représenté sous la forme d’un “bag of words” et chacun des mots se voit affecté un score en fonction de sa fréquence d’apparition au sein d’un vecteur.</p>
<p>Ce score comprend deux composantes :</p>
<ul class="simple">
<li><p>TF : Qui désigne la fréquence de ce mot dans le document</p></li>
<li><p>IDF : Inversement proportionnelle à la fréquence d’apparition dans l’ensemble du corpus (un corpus correspond à un ensemble de document)</p></li>
</ul>
<p>D’un point de vue mathématique, voici la formule associée :</p>
<p><span class="math notranslate nohighlight">\(TF = \frac{\text{Nombre de fois ou le mot apparait dans le document}}{\text{Nombre de mots dans le document}}\)</span></p>
<p><span class="math notranslate nohighlight">\(IDF = log\left(\frac{\text{Nombre de documents}}{\text{Nombre de documents ou le mot apparait}}\right)\)</span></p>
<p><span class="math notranslate nohighlight">\(TFIDF = TF*IDF\)</span></p>
<p>On peut ainsi interpréter ce score de la manière suivante : Plus le mot est fréquent dans le texte, plus il a un poid important, mais plus il est fréquent dans l’ensemble du corpus et plus son poids décroit.
Ainsi les termes récurents dans l’ensemble du corpus (du type déterminant, conjonctions de coordinations ..) auront un poids faible mais les mots caractéristiques d’un texte particulier, avec un réel sens métier, seront d’autant plus pondérés.</p>
<p>Cette métrique est utile pour les taches de classification des documents et permet d’obtenir simplement et rapidement une représentation du texte assez robuste.</p>
<p>Il existe une implémentation dans scikit-learn.</p>
<p>L’utilisation est assez simple, ci-dessous un extrait de code :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;La communauté dfi&quot;</span><span class="p">,</span>
          <span class="s2">&quot;On remercie les rédacteurs ia-z pour ce cours communautaire&quot;</span><span class="p">,</span>
          <span class="s2">&quot;Anis représente la communauté dfi sur Twitch&quot;</span><span class="p">,</span>
          <span class="s2">&quot;La communauté dfi souhaite avoir plus d&#39;émots&quot;</span><span class="p">]</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">idf</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">idf_</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(),</span> <span class="n">idf</span><span class="p">)))</span>

<span class="p">{</span><span class="s1">&#39;anis&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;avoir&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;ce&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;communautaire&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;communauté&#39;</span><span class="p">:</span> <span class="mf">1.2231435513142097</span><span class="p">,</span> 
<span class="s1">&#39;cours&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;dfi&#39;</span><span class="p">:</span> <span class="mf">1.2231435513142097</span><span class="p">,</span> 
<span class="s1">&#39;ia&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;la&#39;</span><span class="p">:</span> <span class="mf">1.2231435513142097</span><span class="p">,</span> 
<span class="s1">&#39;les&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;on&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;plus&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;pour&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;remercie&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;représente&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;rédacteurs&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;souhaite&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;sur&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;twitch&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">,</span> 
<span class="s1">&#39;émots&#39;</span><span class="p">:</span> <span class="mf">1.916290731874155</span><span class="p">}</span>
</pre></div>
</div>
<p>Un problème avec ce type d’approche de représentation vectorielle est la non-prise en compte du contexte au sein des phrases. Nous verrons par la suite d’autres approches tenant compte de cet aspect (notamment avec des modèles plus évolués de type Bert).</p>
<p>En résumé, le score tf-idf est facile à calculer et nous permet de disposer d’une métrique de base pour extraire les termes les plus descriptifs d’un document. Vous pouvez facilement calculer la similarité entre 2 documents en l’utilisant qui fera l’objet d’un article de blog par la suite.</p>
<p>Néanmoins, étant donné que tf-idf est basé sur le  modèle “bag of words” il ne capture donc pas la position dans le texte, la sémantique et les co-occurences dans différents documents. Pour cette raison, le tf-idf n’est utile qu’en tant que feature au niveau lexical. Il ne permet pas de capturer la sémantique (à la différence des words embeddings de type word2vec ou Bert)</p>
<p>Cela dépend donc beaucoup de l’usage que vous voulez faire de TF-IDF !</p>
</div>
<div class="section" id="one-hot-representation-a-name-1-hot-representation-a">
<h3>One-hot representation <a name="1-hot-representation"></a><a class="headerlink" href="#one-hot-representation-a-name-1-hot-representation-a" title="Permalink to this headline">¶</a></h3>
<p>Etant donnée un corpus de texte, nous distinguerons l’ensemble des mots distincts du texte. On se donne une taille maximale N de vocabulaire. Les N-1 mots les plus fréquents du corpus sont indexés par ordre alphabétique. Les autres mots pouvant être attribués au token “OTHER”</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;&quot;</span>
<span class="go">Les Français prennent dès le coup d&#39;envoi le contrôle du milieu de terrain. Dès la troisième minute, Stéphane Guivarc&#39;h, sur une passe de Zinédine Zidane, obtient une occasion de but mais ne cadre pas son tir.</span>

<span class="go">Alors que le match tend à s&#39;équilibrer et que le Brésil se procure plusieurs occasions de buts (notamment un centre au-dessus de la transversale de Roberto Carlos à la 20e minute et une tête de Rivaldo à la 24e minute), les Bleus remettent la pression sur leur adversaire. À la 27e minute, à la suite d&#39;un coup de pied de coin concédé par Roberto Carlos, l&#39;Équipe de France inscrit un but : sur un tir d&#39;Emmanuel Petit, Zinédine Zidane, à 6 m au premier poteau, place une tête piquée en extension au centre du but.</span>

<span class="go">Le gardien français Fabien Barthez réalise une sortie spectaculaire à la 30e minute en passant au-dessus de Ronaldo dans un choc avec l&#39;attaquant brésilien. Alors que la première mi-temps se termine, l&#39;équipe de France inscrit un deuxième but, à nouveau sur corner : tiré par Youri Djorkaeff, le ballon est dévié dans le but brésilien par Zinédine Zidane au premier poteau, lequel se retrouve démarqué à la suite de la glissade de Dunga. </span>
<span class="go">&quot;&quot;&quot;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">count_vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">count_vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="go">109</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">count_vect</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()[:</span><span class="mi">20</span><span class="p">]</span>

<span class="go">array([&#39;20e&#39;, &#39;24e&#39;, &#39;27e&#39;, &#39;30e&#39;, &#39;adversaire&#39;, &#39;alors&#39;, &#39;attaquant&#39;,</span>
<span class="go">       &#39;au&#39;, &#39;avec&#39;, &#39;ballon&#39;, &#39;barthez&#39;, &#39;bleus&#39;, &#39;brésil&#39;, &#39;brésilien&#39;,</span>
<span class="go">       &#39;but&#39;, &#39;buts&#39;, &#39;cadre&#39;, &#39;carlos&#39;, &#39;centre&#39;, &#39;choc&#39;], dtype=object)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;barthez&#39;</span><span class="p">)</span>

<span class="go">10</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">count_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">&#39;barthez&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>

<span class="go">[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span>
<span class="go"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span>
<span class="go"> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</span>

</pre></div>
</div>
</div>
<div class="section" id="word2vec-a-name-word2vec-a">
<h3>Word2Vec <a name="word2vec"></a><a class="headerlink" href="#word2vec-a-name-word2vec-a" title="Permalink to this headline">¶</a></h3>
<p>L’espace de représentation obtenu avec la méthode one-hot est de très grande dimentionalité étant donné qu’il dépend de la taille de notre vocabulaire.</p>
<p>De plus, il ne tient pas compte de l’encapsulation du sens du texte. Plusieurs algorithmes proposent de construire une représentation qui encapsule le sens, correspondant à la notion ‘embeddings’, et en particulier de l’algorithme word2vec.</p>
<p>Plusieurs versions et architectures de l’algorithme existent : L’architecture Skip-Gram cherche à prédire les mots dans le contexte de la cible. A l’inverse l’architecture CBow cherche à prédire un mot en fonction de son contexte.</p>
<p>Différents articles par la suite dans le cadre de ce guide de NLP seront réalisés pour vous présenter ces algorithmes en détails.</p>
</div>
</div>
<div class="section" id="sources">
<h2>Sources<a class="headerlink" href="#sources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://eu.udacity.com/course/deep-learning--ud730">https://eu.udacity.com/course/deep-learning–ud730</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">https://en.wikipedia.org/wiki/Tf–idf</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/NLP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="2_DonneesTextuelles.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Etude des données textuelles</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4_bert.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BERT - Bidirectional Encoder Representations from Transformers</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Communauté IA-Z<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>